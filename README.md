# ğŸš€ Space Station Object Detection with YOLOv8 + AI Voice Assistant

This project leverages **Ultralytics YOLOv8** to detect critical tools and hazards inside a synthetic **space station environment**, combined with an **AI-powered voice assistant** using **Vosk speech recognition** and **Groq LLM** for real-time astronaut assistance. The system is optimized for **high accuracy** and **efficient inference** on edge devices like the **Raspberry Pi**.

---

## ğŸ“ Project Overview

- **Framework**: YOLOv8 by Ultralytics + PyQt5 GUI
- **Dataset**: Falcon's Digital Twin (Space Station Environment)
- **AI Assistant**: Vosk Speech Recognition + Groq API (Llama3-70B)
- **Target**: Real-time object detection + voice-controlled medical assistance
- **Hardware Optimization**: Raspberry Pi (Edge deployment)

---

## âœ¨ Key Features

ğŸ¯ **Real-time Object Detection**: Detect tools, hazards, and critical objects in space station environments  
ğŸ—£ï¸ **Voice Recognition**: Hands-free interaction using Vosk speech recognition  
ğŸ¤– **AI Medical Assistant**: Groq-powered LLM for emergency medical guidance  
ğŸ“± **Modern GUI**: PyQt5 interface with live camera feed and controls  
ğŸ”Š **Text-to-Speech**: Audio feedback for astronaut assistance  
ğŸ¥ **Multi-Model Support**: YOLOv8n, YOLOv8s, and pose estimation variants

---

## ğŸ§ª 1. Environment & Dependency Setup

### âœ… Prerequisites
- Python 3.8+ (via **Anaconda**)
- OS: Windows / MacOS / Linux
- Webcam or video input device
- Internet connection (for Groq API)

### ğŸ“¦ Required Dependencies
```bash
# Core ML/CV libraries
ultralytics
opencv-python
torch
numpy

# GUI and Audio
PyQt5
pyttsx3
sounddevice

# Speech Recognition
vosk

# API requests
requests
python-dotenv
```

### ğŸš€ Setup Instructions

#### 1. Clone this Repository:
```bash
git clone https://github.com/Yajat-prabhakar/yolo-testing.git
cd yolo-testing
```

#### 2. Install Dependencies:
```bash
pip install ultralytics opencv-python torch numpy PyQt5 pyttsx3 sounddevice vosk requests python-dotenv
```

#### 3. Set up Environment Variables:
Create a `.env` file in the project root:
```bash
# .env file
GROQ_API_KEY=your-groq-api-key-here
```

> ğŸ”‘ **Get your Groq API key**: Visit [https://console.groq.com/](https://console.groq.com/) to obtain your free API key

#### 4. Download Required Models:
- **YOLOv8 Models**: Already included (yolov8n.pt, yolov8s.pt, etc.)
- **Vosk Speech Models**: See [Model/README.md](Model/README.md) for download instructions

---

## ğŸ–¼ï¸ 2. Model Setup

### ğŸ“ YOLOv8 Models (Included):
- `yolov8n.pt` - Nano model (fastest, 6.5MB)
- `yolov8s.pt` - Small model (higher accuracy, 22.6MB)  
- `yolov8n-pose.pt` - Pose estimation nano (6.8MB)
- `yolov8s-pose.pt` - Pose estimation small (23.5MB)

### ğŸ¤ Vosk Speech Models (Download Required):
See [Model/README.md](Model/README.md) for detailed setup instructions:
- `vosk-model-en-us-0.22` - High accuracy English model (~70MB)
- `vosk-model-small-en-us-0.15` - Faster, smaller model (~40MB)

---

## ğŸƒâ€â™‚ï¸ 3. Running the Application

### ğŸš€ Launch the Main Application:
```bash
python Main.py
```

### ğŸ® Application Features:

#### **Object Detection Tab**:
- Real-time webcam feed with YOLOv8 detection
- Switch between different YOLO models
- Confidence threshold adjustment
- Save detection results

#### **Voice Assistant Tab**:
- Voice-activated medical assistance
- Speech-to-text using Vosk
- AI-powered responses via Groq (Llama3-70B)
- Text-to-speech feedback

#### **Controls**:
- Start/Stop camera feed
- Toggle voice recognition
- Model selection dropdown
- Confidence slider
- Save image functionality

---

## ğŸ”Š 4. Voice Assistant Usage

### ğŸ¤ Activation:
1. Click "Start Voice Recognition" 
2. Speak clearly into your microphone
3. Ask medical or emergency questions
4. Receive AI-powered audio responses

### ğŸ’¬ Example Voice Commands:
- *"What should I do for a space suit breach?"*
- *"How do I treat a burn in zero gravity?"*
- *"Emergency medical procedures for unconscious crew member"*
- *"First aid for eye injury from debris"*

### ğŸ¤– AI Medical Assistant:
- Powered by **Groq's Llama3-70B-8192** model
- Specialized in **space medicine** and **astronaut emergencies**
- Provides step-by-step medical guidance
- Audio feedback for hands-free operation

---

## âš™ï¸ 5. Configuration

### ğŸ”§ Model Configuration:
Edit model paths in `Main.py`:
```python
# Available models
self.models = {
    "YOLOv8n": "yolov8n.pt",
    "YOLOv8s": "yolov8s.pt", 
    "YOLOv8n-pose": "yolov8n-pose.pt",
    "YOLOv8s-pose": "yolov8s-pose.pt"
}
```

### ğŸ¯ Detection Settings:
```python
# Confidence threshold (adjustable via GUI)
confidence = 0.5  # Default: 50%

# Model inference settings
results = model(frame, conf=confidence)
```

### ğŸ—£ï¸ Voice Settings:
```python
# Vosk model path
model_path = "Model/vosk-model-en-us-0.22"

# Text-to-speech engine
engine = pyttsx3.init()
engine.setProperty('rate', 150)  # Speech rate
```

---

## ğŸ“Š 6. Expected Performance

### ğŸ¯ Object Detection Metrics:
| Model | Size | Speed | Accuracy (mAP@0.5) |
|-------|------|-------|-------------------|
| YOLOv8n | 6.5MB | ~45 FPS | 0.823+ |
| YOLOv8s | 22.6MB | ~30 FPS | 0.850+ |

### ğŸš€ System Requirements:
- **Minimum**: 4GB RAM, integrated graphics
- **Recommended**: 8GB RAM, dedicated GPU
- **Edge Device**: Raspberry Pi 4 (4GB+)

### ğŸ“± Real-time Performance:
- **Camera Feed**: 30 FPS (1080p)
- **Detection Latency**: <50ms (GPU) / <200ms (CPU)
- **Voice Response**: 2-3 seconds (including API call)

---

## ğŸ—‚ï¸ 7. Project Structure

```
yolo-testing/
â”œâ”€â”€ ğŸ“„ README.md                 # This file
â”œâ”€â”€ ğŸ Main.py                   # Main application entry point
â”œâ”€â”€ ğŸ”’ .env                      # Environment variables (API keys)
â”œâ”€â”€ ğŸ“‹ .gitignore               # Git ignore rules
â”œâ”€â”€ ğŸ¤– yolov8n.pt              # YOLOv8 nano model
â”œâ”€â”€ ğŸ¤– yolov8s.pt              # YOLOv8 small model  
â”œâ”€â”€ ğŸ¤– yolov8n-pose.pt         # YOLOv8 pose nano model
â”œâ”€â”€ ğŸ¤– yolov8s-pose.pt         # YOLOv8 pose small model
â””â”€â”€ ğŸ“ Model/                   # Speech recognition models
    â””â”€â”€ ğŸ“– README.md            # Model download instructions
```

---

## ğŸ”§ 8. Troubleshooting

### âŒ Common Issues:

#### **"No module named 'cv2'"**:
```bash
pip install opencv-python
```

#### **"GROQ_API_KEY not found"**:
1. Create `.env` file in project root
2. Add: `GROQ_API_KEY=your-actual-api-key`
3. Restart the application

#### **"Vosk model not found"**:
1. Check [Model/README.md](Model/README.md)
2. Download required Vosk models
3. Extract to `Model/` directory

#### **Camera not detected**:
```python
# Try different camera indices in Main.py
self.cap = cv2.VideoCapture(0)  # Try 0, 1, 2...
```

#### **Poor detection accuracy**:
- Increase confidence threshold
- Switch to YOLOv8s model (higher accuracy)
- Ensure good lighting conditions

---

## ğŸš€ 9. Deployment & Edge Computing

### ğŸ¥§ Raspberry Pi Deployment:
```bash
# Install dependencies
sudo apt update
sudo apt install python3-pip python3-venv

# Create virtual environment  
python3 -m venv yolo_env
source yolo_env/bin/activate

# Install optimized packages
pip install ultralytics[cpu] opencv-python-headless
```

### âš¡ Performance Optimization:
- Use **YOLOv8n** for maximum speed
- Reduce input resolution: `640x640` â†’ `416x416`
- Enable **TensorRT** optimization (NVIDIA devices)
- Use **ONNX** export for CPU inference

---

## ğŸ”¬ 10. Technical Details

### ğŸ§  AI Architecture:
- **Object Detection**: YOLOv8 (PyTorch)
- **Speech Recognition**: Vosk (Offline)
- **Language Model**: Groq Llama3-70B (Cloud API)
- **Text-to-Speech**: pyttsx3 (Local)

### ğŸ“¡ API Integration:
```python
# Groq API configuration
url = "https://api.groq.com/openai/v1/chat/completions"
model = "llama3-70b-8192"
system_prompt = "You are a medical assistant in space..."
```

### ğŸ›ï¸ GUI Framework:
- **Framework**: PyQt5
- **Layout**: Tabbed interface
- **Components**: Video display, controls, voice feedback
- **Threading**: Separate threads for camera, voice, AI processing

---

## ğŸ“ˆ 11. Future Enhancements

ğŸ”® **Planned Features**:
- [ ] Multi-language support (Spanish, Russian, Chinese)
- [ ] Offline LLM integration (Llama2/Mistral)
- [ ] Custom space object training pipeline
- [ ] WebRTC streaming for remote monitoring
- [ ] Integration with space station sensors
- [ ] Advanced pose estimation for crew health monitoring

---

## ğŸ¤ 12. Contributing

We welcome contributions! Please follow these steps:

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Commit changes: `git commit -m "Add feature"`
4. Push to branch: `git push origin feature-name`
5. Submit a pull request

---

## ğŸ“„ 13. License & Credits

### ğŸ“œ License:
This project is licensed under the **MIT License** - see LICENSE file for details.

### ğŸ™ Acknowledgments:
- **Ultralytics** - YOLOv8 framework
- **Groq** - Lightning-fast LLM inference  
- **Vosk** - Offline speech recognition
- **Falcon Digital Twin** - Space station dataset
- **PyQt5** - GUI framework

---

## ğŸ‘¨â€ğŸ’» 14. Contact & Support

**ğŸš€ Team Leader**: Raunaq Adlakha  
ğŸ“§ **Email**: [raunaq.adalkha@gmail.com](mailto:raunaq.adalkha@gmail.com)  
ğŸ”— **GitHub**: [yolo-testing](https://github.com/Yajat-prabhakar/yolo-testing)  
ğŸ’¬ **Issues**: Report bugs and request features via GitHub Issues

---

## âš ï¸ 15. Important Notes

> ğŸ”‘ **API Key Required**: Add your Groq API key to `.env` file to enable AI voice assistant functionality  
> ğŸ¤ **Microphone Access**: Grant microphone permissions for voice recognition  
> ğŸ“¹ **Camera Access**: Ensure webcam is connected and accessible  
> ğŸŒ **Internet Required**: Voice assistant needs internet for Groq API calls  
> ğŸ’¾ **Model Downloads**: Vosk models must be downloaded separately (see Model/README.md)

---

**ğŸš€ This project combines cutting-edge deep learning with edge device deployment to ensure real-time space station monitoring and AI-powered astronaut assistance!**

---

*Last Updated: January 2025*
